import {
  WebSocketGateway,
  WebSocketServer,
  SubscribeMessage,
  OnGatewayConnection,
  OnGatewayDisconnect,
} from '@nestjs/websockets';
import { Server, Socket } from 'socket.io';
import OpenAI from 'openai';
import * as fs from 'fs';
import * as path from 'path';
import { config } from 'dotenv';
import { ChatService } from './chat.service';
import { EvaluationService } from './evaluation.service';

// ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
config();

/**
 * WebSocketã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤
 * ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡ã—ã€Whisper APIã§ãƒ†ã‚­ã‚¹ãƒˆåŒ–
 */
@WebSocketGateway({
  cors: {
    origin: '*',
  },
})
export class AudioGateway implements OnGatewayConnection, OnGatewayDisconnect {
  @WebSocketServer()
  server: Server;

  // OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
  private openai: OpenAI;

  // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ™‚ä¿å­˜ç”¨ãƒãƒƒãƒ•ã‚¡ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã”ã¨ï¼‰
  private audioBuffers: Map<string, Buffer[]> = new Map();

  constructor(
    private chatService: ChatService,
    private evaluationService: EvaluationService,
  ) {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }

  /**
   * ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šæ™‚ã®å‡¦ç†
   */
  handleConnection(client: Socket) {
    console.log(`âœ… Client connected: ${client.id}`);
    this.audioBuffers.set(client.id, []);
  }

  /**
   * ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ‡æ–­æ™‚ã®å‡¦ç†
   * ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
   */
  handleDisconnect(client: Socket) {
    console.log(`âŒ Client disconnected: ${client.id}`);
    this.audioBuffers.delete(client.id);
  }

  /**
   * éŸ³å£°ãƒ‡ãƒ¼ã‚¿å—ä¿¡æ™‚ã®å‡¦ç†
   * è¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆã—ã¦ã€Whisper APIã§ãƒ†ã‚­ã‚¹ãƒˆåŒ–
   */
  @SubscribeMessage('audio-data')
  async handleAudioData(client: Socket, payload: any) {
    console.log('ğŸ¤ Received audio chunk');

    // Blobãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
    const buffer = this.audioBuffers.get(client.id);
    if (buffer) {
      buffer.push(Buffer.from(payload));
    }
  }

  /**
   * éŒ²éŸ³å®Œäº†æ™‚ã®å‡¦ç†
   * ãƒãƒƒãƒ•ã‚¡ã«æºœã‚ãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã€Whisper APIã§æ–‡å­—èµ·ã“ã—
   */
  @SubscribeMessage('audio-complete')
  async handleAudioComplete(client: Socket) {
    console.log('ğŸ”„ Processing audio...');

    const buffer = this.audioBuffers.get(client.id);
    if (!buffer || buffer.length === 0) {
      console.log('âš ï¸ No audio data to process');
      return;
    }

    try {
      // ãƒãƒƒãƒ•ã‚¡ã‚’çµåˆ
      const audioData = Buffer.concat(buffer);

      // ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆWhisper APIã¯ãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›ãŒå¿…è¦ï¼‰
      const tempDir = path.join(__dirname, '../../temp');
      if (!fs.existsSync(tempDir)) {
        fs.mkdirSync(tempDir, { recursive: true });
      }

      const tempFilePath = path.join(tempDir, `audio-${client.id}-${Date.now()}.webm`);
      fs.writeFileSync(tempFilePath, audioData);
      console.log(`ğŸ’¾ Saved audio file: ${tempFilePath}`);

      // Whisper APIã§æ–‡å­—èµ·ã“ã—
      const transcription = await this.openai.audio.transcriptions.create({
        file: fs.createReadStream(tempFilePath),
        model: 'whisper-1',
        language: 'ja', // æ—¥æœ¬èªã‚’æŒ‡å®šï¼ˆè‡ªå‹•æ¤œå‡ºã‚‚å¯èƒ½ï¼‰
      });

      console.log(`ğŸ“ Transcription: ${transcription.text}`);

      // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«çµæœã‚’é€ä¿¡
      client.emit('transcription', {
        text: transcription.text,
        timestamp: new Date().toISOString(),
      });

      // AIå¿œç­”ã‚’ç”Ÿæˆ
      const response = await this.chatService.chat(client.id, transcription.text);

      // ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’é€ä¿¡
      client.emit('ai-response', {
        message: response,
        timestamp: new Date().toISOString(),
      });

      // éŸ³å£°ã‚‚é€ä¿¡
      const audioBuffer = await this.textToSpeech(response);
      client.emit('ai-audio', {
        audio: audioBuffer.toString('base64'),
      });

      // ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
      fs.unlinkSync(tempFilePath);
      console.log('ğŸ—‘ï¸ Temp file deleted');

      // ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
      this.audioBuffers.set(client.id, []);

    } catch (error) {
      console.log('Error processing audio:', error);
      client.emit('processing-error', {
        message: 'Failed to process audio',
        error: error.message,
      });
    }
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡æ™‚ã®å‡¦ç†
   * GPT-4ã§å¿œç­”ã‚’ç”Ÿæˆã—ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«è¿”ã™
   */
  @SubscribeMessage('text-message')
  async handleTextMessage(client: Socket, payload: { message: string }) {
    console.log(`ğŸ’¬ Received text message from ${client.id}: ${payload.message}`);

    try {
      // ChatServiceã§å¿œç­”ã‚’ç”Ÿæˆ
      const response = await this.chatService.chat(client.id, payload.message);

      // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å¿œç­”ã‚’é€ä¿¡
      client.emit('ai-response', {
        message: response,
        timestamp: new Date().toISOString(),
      });

      // éŸ³å£°ã‚‚é€ä¿¡
      const audioBuffer = await this.textToSpeech(response);
      client.emit('ai-audio', {
        audio: audioBuffer.toString('base64'),
      });

    } catch (error) {
      console.log('Error in text chat:', error);
      client.emit('processing-error', {
        message: 'Failed to generate response',
        error: error.message,
      });
    }
  }

  /**
   * é¢æ¥é–‹å§‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†
   * - AIé¢æ¥å®˜ã®æŒ¨æ‹¶ã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨éŸ³å£°ã§è¿”ã™
   * - éŸ³å£°ç”Ÿæˆã«å¤±æ•—ã—ã¦ã‚‚ãƒ†ã‚­ã‚¹ãƒˆã¯å¿…ãšè¿”ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
   */
  @SubscribeMessage('start-interview')
  async handleStartInterview(client: Socket): Promise<void> {
    const clientId = client.id;
    console.log(`ğŸ¬ é¢æ¥é–‹å§‹: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆID=${clientId}`);

    try {
      // AIé¢æ¥å®˜ã®æŒ¨æ‹¶æ–‡ã‚’ç”Ÿæˆ
      const greeting = await this.chatService.startInterview(clientId);
      console.log(`âœ… æŒ¨æ‹¶æ–‡ç”Ÿæˆå®Œäº†: ${greeting.substring(0, 30)}...`);

      // ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’é€ä¿¡ï¼ˆå¿…ãšå®Ÿè¡Œï¼‰
      client.emit('ai-response', {
        message: greeting,
        timestamp: new Date().toISOString(),
      });

      // éŸ³å£°ã‚‚é€ä¿¡
      const audioBuffer = await this.textToSpeech(greeting);
      client.emit('ai-audio', {
        audio: audioBuffer.toString('base64'),
      });

    } catch (error) {
      console.error('âŒ é¢æ¥é–‹å§‹ã‚¨ãƒ©ãƒ¼:', error.message);
      client.emit('processing-error', {
        message: 'Failed to start interview',
        error: error.message,
      });
    }
  }

  /**
   * é¢æ¥ã‚’çµ‚äº†ã—ã¦è©•ä¾¡ã‚’å®Ÿè¡Œ
   */
  @SubscribeMessage('end-interview')
  async handleEndInterview(client: Socket): Promise<void> {
    const clientId = client.id;
    console.log('ğŸ é¢æ¥çµ‚äº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆ:', clientId);

    try {
      // ä¼šè©±å±¥æ­´ã‚’å–å¾—
      const history = this.chatService.getHistory(clientId);

      if (history.length === 0) {
        console.log('âš ï¸ ä¼šè©±å±¥æ­´ãŒç©ºã§ã™');
        client.emit('evaluation-error', {
          error: 'ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“',
        });
        return;
      }

      console.log('ğŸ“Š è©•ä¾¡ã‚’é–‹å§‹...ï¼ˆä¼šè©±æ•°: ' + history.length + 'ï¼‰');

      // è©•ä¾¡ã‚’å®Ÿè¡Œ
      const evaluation = await this.evaluationService.evaluateCandidate(
        clientId,
        history,
      );

      console.log('âœ… è©•ä¾¡å®Œäº†:', evaluation.scores.overall, 'ç‚¹');

      // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«è©•ä¾¡çµæœã‚’é€ä¿¡
      client.emit('evaluation-complete', evaluation);

      // ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
      // this.chatService.clearHistory(clientId);

    } catch (error) {
      console.error('âŒ è©•ä¾¡ã‚¨ãƒ©ãƒ¼:', error);
      client.emit('evaluation-error', {
        error: 'è©•ä¾¡ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ',
      });
    }
  }

  /**
   * è©•ä¾¡çµæœã‚’å–å¾—
   */
  @SubscribeMessage('get-evaluation')
  handleGetEvaluation(client: Socket): void {
    const clientId = client.id;
    const evaluation = this.evaluationService.getEvaluation(clientId);

    if (evaluation) {
      client.emit('evaluation-result', evaluation);
    } else {
      client.emit('evaluation-error', {
        error: 'è©•ä¾¡çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
      });
    }
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ï¼ˆTTSï¼‰
   * @param text - éŸ³å£°åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
   * @returns éŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆMP3å½¢å¼ã®Bufferï¼‰
   */
  private async textToSpeech(text: string): Promise<Buffer> {
    try {
      console.log('ğŸ”Š TTSç”Ÿæˆé–‹å§‹:', text.substring(0, 50) + '...');

      const mp3 = await this.openai.audio.speech.create({
        model: 'tts-1',        // æ¨™æº–å“è³ªï¼ˆé€Ÿã„ãƒ»å®‰ã„ï¼‰
        voice: 'nova',         // å¥³æ€§ã®å£°ï¼ˆè¦ªã—ã¿ã‚„ã™ã„ï¼‰
        input: text,
        speed: 1.0,            // é€šå¸¸é€Ÿåº¦
      });

      const buffer = Buffer.from(await mp3.arrayBuffer());
      console.log('âœ… TTSç”ŸæˆæˆåŠŸ ã‚µã‚¤ã‚º:', buffer.length, 'bytes');
      return buffer;

    } catch (error) {
      console.error('âŒ TTSç”Ÿæˆã‚¨ãƒ©ãƒ¼:', error.message);
      throw error;
    }
  }
}